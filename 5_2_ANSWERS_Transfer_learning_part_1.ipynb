{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mattshanevdberg/ML-FruitPunchAI_BootCamp/blob/main/5_2_ANSWERS_Transfer_learning_part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLCnueuqbuVn",
        "outputId": "c1ad0328-513d-4706-c909-1de7acc30988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.4 MB 7.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 56.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 7.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 61.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 70.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 306 kB 7.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 243 kB 72.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 69.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 79.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 65.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 160 kB 60.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 192 kB 72.5 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transfer Learning part 1**\n",
        "\n",
        "First we will have a look at the huggingface library. This is a library full of large pretrained models that are easily to be installed and used. Huggingface has a large amount of NLP (Natural Language Processing) algorithms but also offers alorithms for audio and vision processing. Check out their site for all the available models:\n",
        "https://huggingface.co/models\n",
        "\n",
        "Here below we give an example of an algorithm named \"bert-base-NER\". bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task (Named Entity Recognition). It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC).\n",
        "\n",
        "For more info on named entity recognition you can check out this paper https://aclanthology.org/W03-0419.pdf"
      ],
      "metadata": {
        "id": "BFXmFknYcFM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
        "\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "example = \"I'm Dorian from Utrecht and I work for Fruitpunch AI.\"\n",
        "\n",
        "ner_results = nlp(example)\n",
        "print(ner_results)"
      ],
      "metadata": {
        "id": "DaCX-uuIxpQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we really start transfer learning we want to show you how usefull pre-trained models can be, and potentially can save you a lot of time. For the first exercise we'll take a model from huggingface and see how well it performs vs our own build model from scratch. Please note: This is not yet transfer learning because we are not re-training a model."
      ],
      "metadata": {
        "id": "ljUlEbWudXPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "!git clone https://github.com/fruitpunch-ai-code/epoch-14.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYdQt3DQe-dR",
        "outputId": "394a7cbf-397e-41bc-b32d-301cd83b79c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'epoch-14' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn import model_selection\n",
        "import nltk\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "#Set Random seed\n",
        "np.random.seed(500)\n",
        "\n",
        "df = pd.read_csv('/content/epoch-14/Challenges/reviews.csv', encoding='latin-1')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "YwlgSr64fCGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will train a NLP from scratch to recognise the sentiment in amazon reviews. The model will have to determine if the review is positive or negative. \n"
      ],
      "metadata": {
        "id": "OtTbXNZzkoK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Data Pre-processing - This will help in getting better results through the classification algorithms\n",
        "Corpus = df.copy()\n",
        "# Step 1a : Remove blank rows if any.\n",
        "Corpus['text'].dropna(inplace=True)\n",
        "\n",
        "# Step - 1b : Change all the text to lower case. This is required as python interprets 'dog' and 'DOG' differently\n",
        "Corpus['text'] = [entry.lower() for entry in Corpus['text']]\n",
        "\n",
        "# Step - 1c : Tokenization : In this each entry in the corpus will be broken into set of words\n",
        "Corpus['text']= [word_tokenize(entry) for entry in Corpus['text']]\n",
        "\n",
        "# Step - 1d : Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.\n",
        "\n",
        "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
        "tag_map = defaultdict(lambda : wn.NOUN)\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "\n",
        "for index,entry in enumerate(Corpus['text']):\n",
        "    # Declaring Empty List to store the words that follow the rules for this step\n",
        "    Final_words = []\n",
        "    # Initializing WordNetLemmatizer()\n",
        "    word_Lemmatized = WordNetLemmatizer()\n",
        "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
        "    for word, tag in pos_tag(entry):\n",
        "        # Below condition is to check for Stop words and consider only alphabets\n",
        "        if word not in stopwords.words('english') and word.isalpha():\n",
        "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
        "            Final_words.append(word_Final)\n",
        "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
        "    Corpus.loc[index,'text_final'] = str(Final_words)\n",
        "\n",
        "# Step 2: Split the model into Train and Test Data set\n",
        "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text_final'],Corpus['label'],test_size=0.3)\n",
        "\n",
        "# Step 3: Label encode the target variable  - This is done to transform Categorical data of string type in the data set into numerical values\n",
        "Encoder = LabelEncoder()\n",
        "Train_Y = Encoder.fit_transform(Train_Y)\n",
        "Test_Y = Encoder.fit_transform(Test_Y)\n",
        "\n",
        "# Step 4: Vectorize the words by using TF-IDF Vectorizer - This is done to find how important a word in document is in comparison to the corpus\n",
        "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
        "Tfidf_vect.fit(Corpus['text_final'])\n",
        "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
        "Test_X_Tfidf = Tfidf_vect.transform(Test_X)"
      ],
      "metadata": {
        "id": "bPIKv1NDfJZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Now run ML algorithm to classify the text\n",
        "\n",
        "# Classifier - Algorithm - Naive Bayes\n",
        "from sklearn import naive_bayes\n",
        "from sklearn.metrics import accuracy_score\n",
        "# fit the training dataset on the classifier\n",
        "Naive = naive_bayes.MultinomialNB()\n",
        "Naive.fit(Train_X_Tfidf,Train_Y)\n",
        "# predict the labels on validation dataset\n",
        "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)\n",
        "\n",
        "# Classifier - Algorithm - SVM\n",
        "from sklearn import svm\n",
        "# fit the training dataset on the classifier\n",
        "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "SVM.fit(Train_X_Tfidf,Train_Y)\n",
        "# predict the labels on validation dataset\n",
        "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
        "\n",
        "# Classifier - Algorithm - Random Forest Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# fit the training dataset on the classifier\n",
        "clf = RandomForestClassifier(n_estimators=400, max_depth=20, random_state=0)\n",
        "clf.fit(Train_X_Tfidf,Train_Y)\n",
        "# predict the labels on validation dataset\n",
        "predictions_clf = clf.predict(Test_X_Tfidf)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"Random Forest Classifier Accuracy Score -> \",accuracy_score(predictions_clf, Test_Y)*100)\n",
        "\n",
        "# Classifier - Algorithm - AdaBoost Classifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "# fit the training dataset on the classifier\n",
        "adaclf = AdaBoostClassifier(n_estimators=800, random_state=0)\n",
        "adaclf.fit(Train_X_Tfidf,Train_Y)\n",
        "# predict the labels on validation dataset\n",
        "predictions_adaclf = adaclf.predict(Test_X_Tfidf)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"AdaBoost Classifier Accuracy Score -> \",accuracy_score(predictions_adaclf, Test_Y)*100)\n",
        "\n",
        "# Classifier - Algorithm - Linear SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "# fit the training dataset on the classifier\n",
        "svc = LinearSVC(random_state=0, tol=1e-5)\n",
        "svc.fit(Train_X_Tfidf,Train_Y)\n",
        "# predict the labels on validation dataset\n",
        "predictions_svc = svc.predict(Test_X_Tfidf)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"Linear SVC Accuracy Score -> \",accuracy_score(predictions_svc, Test_Y)*100)\n",
        "\n",
        "# Classifier - Algorithm - Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# fit the training dataset on the classifier\n",
        "lr = LogisticRegression()\n",
        "lr.fit(Train_X_Tfidf,Train_Y)\n",
        "# predict the labels on validation dataset\n",
        "predictions_lr = lr.predict(Test_X_Tfidf)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"Logistic Regression Accuracy Score -> \",accuracy_score(predictions_lr, Test_Y)*100)\n",
        "\n",
        "# Classifier - Algorithm - MLP Classifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "# fit the training dataset on the classifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n",
        "mlp.fit(Train_X_Tfidf,Train_Y)\n",
        "# predict the labels on validation dataset\n",
        "predictions_nn = mlp.predict(Test_X_Tfidf)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"MLPClassifier Accuracy Score -> \",accuracy_score(predictions_nn, Test_Y)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n73yZMIKfWY-",
        "outputId": "4aad0c44-0ea7-4025-a08b-db1464416dfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy Score ->  81.96666666666667\n",
            "SVM Accuracy Score ->  84.36666666666667\n",
            "Random Forest Classifier Accuracy Score ->  81.63333333333334\n",
            "AdaBoost Classifier Accuracy Score ->  79.76666666666667\n",
            "Linear SVC Accuracy Score ->  83.43333333333334\n",
            "Logistic Regression Accuracy Score ->  85.0\n",
            "MLPClassifier Accuracy Score ->  81.39999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best model reaches an accuracy of 85% let's see if we can top that with a model from huggingface"
      ],
      "metadata": {
        "id": "X1bILW9xmxC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Assignment 1**\n",
        "\n",
        "A) Search the huggingface library for a model that can classify the reviews in this dataset on positive or negative. And run it on the data.\n",
        "\n",
        "B) Eveluate your transfered model. Does it outperform the models build from scratch?"
      ],
      "metadata": {
        "id": "0VJwDsfKnSUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
        "\n",
        "model_name = \"aychang/roberta-base-imdb\"\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "nlp = pipeline(\"sentiment-analysis\", model=model_name, tokenizer=model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL00cfekgpar",
        "outputId": "97d67751-bb98-4819-fb63-63be706a5a0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at aychang/roberta-base-imdb were not used when initializing RobertaForQuestionAnswering: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at aychang/roberta-base-imdb and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the model we took here only takes lists as input\n",
        "list_of_text = df.text.tolist()"
      ],
      "metadata": {
        "id": "S0PRtrM_gskw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To speed things up a little bit grab a subset of the data\n",
        "import tensorflow as tf\n",
        "with tf.device('/device:GPU:0'):\n",
        "  results = nlp(list_of_text[0:1000])"
      ],
      "metadata": {
        "id": "mo1drWIYgu7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the predictions from the results\n",
        "pred = []\n",
        "for i in range(1000):\n",
        "  if results[i]['label'] == 'pos':\n",
        "    pred.append(1)\n",
        "  if results[i]['label'] == 'neg':\n",
        "    pred.append(0)"
      ],
      "metadata": {
        "id": "gr14GpVEg3rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy score but first transform labels to numbers\n",
        "df['label'] = Encoder.fit_transform(df['label'])\n",
        "print(\"Transfered model Accuracy Score -> \",accuracy_score(pred, df['label'][0:1000])*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd-NH8GEg68y",
        "outputId": "4347b76d-a99c-42d5-e555-e77dc250aebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transfered model Accuracy Score ->  92.80000000000001\n"
          ]
        }
      ]
    }
  ]
}
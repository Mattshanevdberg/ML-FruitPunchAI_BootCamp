{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mattshanevdberg/ML-FruitPunchAI_BootCamp/blob/main/1_3_Convolutional_Neural_Networks_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "zpOMNIqo_OkM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Networks\n",
        "This challenge is going to be a bit more, well... challenging. You are going to process image data and use Convolutional Neural Networks. These types of algorithms are designed to use multidimensional data where the spatial relations between the dimensions are relevant. Or in other words, good with pretty pictures.\n",
        "\n",
        "Today we are going to use the MNIST dataset, probably the most famous dataset in all of AI. It consists of 28x28 pixel pictures of handwritten numbers in greyscale (black & white).\n",
        "\n",
        "We are going to use some real Deep Learning here that benefits from GPU acceleration. Luckily Colab can provide us with a free GPU to train the neural networks faster. You can activate GPU acceleration in the Runtime tab: Runtime -> Change runtime type -> Select GPU from the dropdown\n",
        "\n"
      ],
      "metadata": {
        "id": "wFpJ9cbY_MgB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i_F82xoK_INS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30124f9a-b827-4883-d74c-eb3b961560c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n",
            "[  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208\n",
            " 211 218 224 223 219 215 224 244 159   0]\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "from keras.datasets import mnist, fashion_mnist\n",
        "#(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_train[0, 15,])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View example digit\n",
        "index = np.random.randint(0,60000)\n",
        "plt.imshow(X_train[index])\n",
        "print(y_train[index])"
      ],
      "metadata": {
        "id": "ELbl-BK__Yhk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "864d457f-502c-4dd6-c1f1-874d7475c4df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reshaping the data\n",
        "CNNs expect the data to be in a specific shape depending on the number of dimensions. Since we work with image data, we will use 2D CNN layers. The expected shape that would be as follows (number_of_samples, dimension_size_1, dimension_size_2, channel). Looking at X_train. The shape above the first 3 is easy to derive. The number of training samples is 60000 and the images are 28*28 and therefore those would be the two dimension sizes. Channel would basically be the number of spectral wavelengths are in the data. For colored images this is 3 (red, green, blue) for greyscale images this would just be 1, and for multispectral satellite images, this could be many more. In this case, we have black and white images, so we end up with the desired X_train shape of (60000,28,28,1). This is not the shape of our data as can be seen above, that would be (60000,28,28). This is because it has the intensity value in the second dimension instead of it being just a coordinate and an extra dimension for the intensity. Luckily we can easily reshape the data to our desired format."
      ],
      "metadata": {
        "id": "LZvsMNxd_ewY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put it into suitable shape\n",
        "X_train = X_train.reshape(60000,28,28,1)\n",
        "X_test = X_test.reshape(10000,28,28,1)\n",
        "print(X_train[0, 15,])\n",
        "print(X_train.shape[1:])"
      ],
      "metadata": {
        "id": "uDjrnzVa_fUr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9b3fc03-4a3c-434d-b72f-d25136878f31"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0]\n",
            " [  3]\n",
            " [  0]\n",
            " [  0]\n",
            " [  0]\n",
            " [  0]\n",
            " [  0]\n",
            " [  0]\n",
            " [  0]\n",
            " [ 62]\n",
            " [145]\n",
            " [204]\n",
            " [228]\n",
            " [207]\n",
            " [213]\n",
            " [221]\n",
            " [218]\n",
            " [208]\n",
            " [211]\n",
            " [218]\n",
            " [224]\n",
            " [223]\n",
            " [219]\n",
            " [215]\n",
            " [224]\n",
            " [244]\n",
            " [159]\n",
            " [  0]]\n",
            "(28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use one hot encoding to covert the labels\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "lb = LabelBinarizer()\n",
        "print(y_train[0])\n",
        "lb.fit(y_train)\n",
        "y_train = lb.transform(y_train)\n",
        "y_test = lb.transform(y_test)\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "id": "xM51x7pm_kFZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68957d30-89d1-4db3-a25f-d24331a905e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "[0 0 0 0 0 0 0 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architecture\n",
        "Now for the architecture\n",
        "\n",
        "- This time we defined the input in a separate Input Layer, which results in exactly the same as when you define input_shape in the first layer.\n",
        "- Then we do a single Convolutional layer\n",
        "- The last layer should stay a Dense layer with the number of classes for neurons just like with the previous. However, we can not just push 2D data into a 1D Dense layer therefore we need to Flatten the data using a Flatten layer.\n",
        "\n",
        "\n",
        "Configuring the Conv2D layers will require some fiddling, here is the documentation:\n",
        "https://keras.io/api/layers/convolution_layers/convolution2d/\n",
        "<br><br>\n",
        "You should at least define a **filter size**. To keep it simple, this number determines the amount of information that will be collected per output \"pixel\" in the output. So a higher number means more space for information but also longer compute times and larger model size. \n",
        "<br><br>\n",
        "Then you should also define a **kernel size** (also known as window) Which basically determines how much information is condensed into an output \"pixel\". This means that larger kernels result in a larger segment of the image will be cramped into a smaller resulting image. So here larger means less information pushed to the next layer. \n",
        "\n",
        "**Warning:** When adding Conv layers and changing kernel sizes you impact the shape of de \"image\" that is passed to the next layers, often making it smaller. This means that when you stack too many layers and/or use large kernel sizes, the image at some point will become smaller than 0x0 resulting in a crash. You can simply calculate the effect each propagating layer will have on the image but you can also pay extra attention to the model summary since that will show the resulting image dimensions after each layer.\n",
        " \n"
      ],
      "metadata": {
        "id": "siK3d9Wr_nA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.layers import (Input, Conv2D, MaxPooling2D, Flatten, \n",
        "                                     Dense, Dropout, BatchNormalization, Activation)"
      ],
      "metadata": {
        "id": "JmbIVsFd_kj1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_cnn(input_shape, num_classes):\n",
        "    # Build the architecture\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # first CONV => RELU => CONV => RELU \n",
        "    model.add(Conv2D(30, 3, padding=\"same\",\n",
        "        input_shape=input_shape))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Conv2D(30, 3, padding=\"same\"))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    # second CONV => RELU => CONV => RELU\n",
        "    model.add(Conv2D(20, 3, padding=\"same\"))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Conv2D(20, 3, padding=\"same\"))\n",
        "    model.add(Activation(\"relu\"))\n",
        "        # third CONV => RELU => CONV => RELU\n",
        "    model.add(Conv2D(5, 3, padding=\"same\"))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Conv2D(5, 3, padding=\"same\"))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    # first (and only) set of FC => RELU layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    # softmax classifier\n",
        "    model.add(Dense(num_classes))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "W1dUl9WS_pxO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the network\n",
        "For the optimizer we will use classic Stochastic Gradient Descent. There exist more advanced optimizers like Adam, but we will skip those for now. The loss defines how we punish the network for a mistake. Without going into the details Categorical Crossentropy is generally used with classification."
      ],
      "metadata": {
        "id": "KvIwrB6H_09e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "batch_size = 32\n",
        "input_shape = X_train.shape[1:]\n",
        "print(input_shape)\n",
        "num_classes = y_train.shape[-1]\n",
        "\n",
        "model = define_cnn(input_shape, num_classes)\n",
        "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Train and test\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "mIA9lYi6_1dg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8798df28-d987-4c45-f9a5-c410810b6edc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28, 1)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 30)        300       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 28, 28, 30)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 28, 28, 30)        8130      \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 28, 28, 30)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 28, 28, 20)        5420      \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 28, 28, 20)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 28, 28, 20)        3620      \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 28, 28, 20)        0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 28, 28, 5)         905       \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 28, 28, 5)         0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 28, 28, 5)         230       \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 28, 28, 5)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3920)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               2007552   \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,031,287\n",
            "Trainable params: 2,031,287\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1875/1875 [==============================] - 18s 5ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd2d0228350>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "metadata": {
        "id": "TCp89GeB_3C4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86adf5d0-8ffd-4c3f-db5b-c767b4877755"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: nan\n",
            "Test accuracy: 0.10000000149011612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Assignment:** Tune the CNN\n",
        "2.1: Add convolutional layers, change kernel and filter sizes to get to a 98% accuracy on the test dataset\n",
        "\n",
        "2.2: Go back to the data loader and change the MNIST loader to the Fashion MNIST loader \"fashion_mnist.load_data()\". Tune the parameters until you reach an accuracy of 87% on the test dataset.\n",
        "\n",
        "2.3 (Advanced): Include max-pooling, dropout, and batch normalization layers to reach a test accuracy of 90% on the test set.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZKVdjm3AAM2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def define_cnn(input_shape, num_classes):\n",
        "    # Build the architecture\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # first CONV => RELU => CONV => RELU \n",
        "    model.add(Conv2D(30, 3, padding=\"same\",\n",
        "        input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(Conv2D(30, 3, padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D())\n",
        "    # second CONV => RELU => CONV => RELU\n",
        "    model.add(Conv2D(20, 3, padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    #model.add(MaxPooling2D())\n",
        "    model.add(Conv2D(20, 3, padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    #model.add(MaxPooling2D())\n",
        "        # third CONV => RELU => CONV => RELU\n",
        "    model.add(Conv2D(5, 3, padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    #model.add(MaxPooling2D())\n",
        "    model.add(Conv2D(5, 3, padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    #model.add(MaxPooling2D())\n",
        "      # first CONV => RELU => CONV => RELU\n",
        "    model.add(Conv2D(5, 3, padding=\"same\"))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    #model.add(MaxPooling2D())\n",
        "    # first (and only) set of FC => RELU layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    # softmax classifier\n",
        "    model.add(Dense(num_classes))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "\n",
        "    model.summary()\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "EyRrdvVam3kd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "input_shape = X_train.shape[1:]\n",
        "print(input_shape)\n",
        "num_classes = y_train.shape[-1]\n",
        "\n",
        "model = define_cnn(input_shape, num_classes)\n",
        "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Train and test\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjrbgkmVm6Rk",
        "outputId": "ed938281-b617-40ac-9441-a9d27572c9f4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28, 1)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 30)        300       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 28, 28, 30)       120       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 28, 28, 30)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 30)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 14, 14, 30)        8130      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 14, 14, 30)       120       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 14, 14, 30)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 7, 7, 30)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 7, 7, 20)          5420      \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 7, 7, 20)         80        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 7, 7, 20)          0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 7, 7, 20)          3620      \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 7, 7, 20)         80        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 7, 7, 20)          0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 7, 7, 5)           905       \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 7, 7, 5)          20        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 7, 7, 5)           0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 7, 7, 5)           230       \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 7, 7, 5)          20        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 7, 7, 5)           0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 7, 7, 5)           230       \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 7, 7, 5)           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 245)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               125952    \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 150,357\n",
            "Trainable params: 150,137\n",
            "Non-trainable params: 220\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 22s 7ms/step - loss: 0.5506 - accuracy: 0.8071 - val_loss: 0.3920 - val_accuracy: 0.8538\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3448 - accuracy: 0.8742 - val_loss: 0.3638 - val_accuracy: 0.8649\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3025 - accuracy: 0.8893 - val_loss: 0.3483 - val_accuracy: 0.8713\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2788 - accuracy: 0.8980 - val_loss: 0.3118 - val_accuracy: 0.8851\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2599 - accuracy: 0.9039 - val_loss: 0.2953 - val_accuracy: 0.8945\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2452 - accuracy: 0.9103 - val_loss: 0.2894 - val_accuracy: 0.8947\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.2335 - accuracy: 0.9138 - val_loss: 0.2918 - val_accuracy: 0.8938\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2223 - accuracy: 0.9182 - val_loss: 0.2867 - val_accuracy: 0.8975\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2135 - accuracy: 0.9214 - val_loss: 0.2975 - val_accuracy: 0.8927\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2072 - accuracy: 0.9231 - val_loss: 0.2693 - val_accuracy: 0.9051\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0430212b10>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnOZKqyWnBID",
        "outputId": "96056f8f-38d8-475d-cd33-68126646d18f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.2692696154117584\n",
            "Test accuracy: 0.9050999879837036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hJgeOHZopAU_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}